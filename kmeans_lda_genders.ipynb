{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.util import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import re\n",
    "\n",
    "from scipy.cluster.hierarchy import ward, dendrogram, linkage\n",
    "#from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_profit_dict = {'V. Human Services': 'human_services', 'IV. Health':'health', 'II. Education':'education',\n",
    "                  'III. Environment and Animals':'env_animals', 'VII. Public, Societal Benefit':'public_benefit',\n",
    "                  'I. Arts, Culture, and Humanities':'arts',\n",
    "                   'X. Unknown, Unclassified or Other': 'other', 'IX. Mutual/Membership Benefit':'other',\n",
    "                 'VI. International, Foreign Affairs' : 'foreign_affairs', 'VIII. Religion Related':'religion'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'rowkey', 'story', 'storyclean', 'ipaddress', 'charityname',\n",
       "       'ein', 'nonprofit_focus', 'ntee_short', 'ntee_desc', 'age',\n",
       "       'genderclean', 'gender', 'zip', 'state', 'votes_lower', 'votes_higher',\n",
       "       'rankpercentile_lower', 'rankpercentile_higher', 'domain', 'extention',\n",
       "       'username'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('All_Data_Combined_v3.csv', encoding = 'latin1')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['nonprofit_cat'] = [non_profit_dict[focus.strip()] for focus in df.nonprofit_focus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rankpercentile_lower</th>\n",
       "      <th>votes_higher</th>\n",
       "      <th>votes_lower</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429557</td>\n",
       "      <td>211.209877</td>\n",
       "      <td>30.382716</td>\n",
       "      <td>2016.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.330079</td>\n",
       "      <td>1251.005892</td>\n",
       "      <td>69.204691</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.100861</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.437884</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2016.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.693727</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.972940</td>\n",
       "      <td>11200.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>2017.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       rankpercentile_lower  votes_higher  votes_lower         year\n",
       "count             81.000000     81.000000    81.000000    81.000000\n",
       "mean               0.429557    211.209877    30.382716  2016.444444\n",
       "std                0.330079   1251.005892    69.204691     0.500000\n",
       "min                0.000000      0.000000     0.000000  2016.000000\n",
       "25%                0.100861      1.000000     1.000000  2016.000000\n",
       "50%                0.437884      4.000000     3.000000  2016.000000\n",
       "75%                0.693727     42.000000    23.000000  2017.000000\n",
       "max                0.972940  11200.000000   506.000000  2017.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('nonprofit_cat').describe().iloc[:,2:].ix['arts'] #choose any nonprofit_cat to look at stats per category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Female              1041\n",
       "Male                 280\n",
       "Decline to State       2\n",
       "Name: genderclean, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genderclean.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.genderclean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Clustering using Kmeans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females = df[df.genderclean == 'Female']\n",
    "stories = females.storyclean\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizer(doc):\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "    \n",
    "    tokenized = []\n",
    "    words = word_tokenize(doc)\n",
    "    for w in words:\n",
    "        if re.match(\"([a-zA-Z]+'[a-zA-Z]+)$|([a-zA-Z]+')$|([a-zA-Z]+)$\",w):\n",
    "            if w not in stop:\n",
    "                tokenized.append(w)\n",
    "    return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters =5\n",
    "tf_idf = TfidfVectorizer(encoding='latin-1',max_df=0.9, min_df=2, tokenizer= mytokenizer, ngram_range=(1,1), max_features=100000)\n",
    "tf_idf.fit(stories)\n",
    "\n",
    "X = tf_idf.fit_transform(stories).toarray()\n",
    "terms = tf_idf.get_feature_names()\n",
    "\n",
    "km = KMeans(n_clusters).fit(X)\n",
    "km_result = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: student, school, education, child, program, art, wa, college, girl, music,\n",
      "\n",
      "Cluster 1 words: cancer, wa, family, diagnosed, treatment, research, year, life, patient, disease,\n",
      "\n",
      "Cluster 2 words: dog, animal, rescue, pet, shelter, wa, home, help, foster, cat,\n",
      "\n",
      "Cluster 3 words: wa, child, life, help, family, year, give, community, people, time,\n",
      "\n",
      "Cluster 4 words: horse, riding, equine, carrot, wa, golden, casey, ranch, ride, care,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :10]: #replace 6 with n words per cluster\n",
    "        print(' %s,' % terms[ind],end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of documents per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    735\n",
       "0    117\n",
       "1     98\n",
       "2     63\n",
       "4     28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(km.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males = df[df.gender == 'Male']\n",
    "stories = males.storyclean\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizer(doc):\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stop += ['.', ',', '(', ')', \"'\", '\"', 'wa']\n",
    "    \n",
    "    tokenized = []\n",
    "    words = word_tokenize(doc)\n",
    "    for w in words:\n",
    "        if re.match(\"([a-zA-Z]+'[a-zA-Z]+)$|([a-zA-Z]+')$|([a-zA-Z]+)$\",w):\n",
    "            if w not in stop:\n",
    "                tokenized.append(w)\n",
    "    return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "tf_idf = TfidfVectorizer(encoding='latin-1',max_df=0.5, min_df=0.01, tokenizer= mytokenizer, ngram_range=(1,1), max_features=10000)\n",
    "tf_idf.fit(stories)\n",
    "\n",
    "X = tf_idf.fit_transform(stories).toarray()\n",
    "terms = tf_idf.get_feature_names()\n",
    "\n",
    "km = KMeans(n_clusters).fit(X)\n",
    "km_result = km.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "\n",
      "Cluster 0 words: food, meal, shelter, need, donated, city, benefit, woman, support, scholarship, dignity, pet, time, large, technology, obstacle, baby, mission, card, cause,\n",
      "\n",
      "Cluster 1 words: community, child, world, one, giving, give, people, u, school, program, need, student, day, ha, project, every, work, time, could, new,\n",
      "\n",
      "Cluster 2 words: cancer, kid, cure, time, disease, one, breast, day, truck, research, many, mom, camp, raise, im, people, child, money, friend, want,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "print()\n",
    "\n",
    "#sort cluster centers by proximity to centroid\n",
    "order_centroids = km.cluster_centers_.argsort()[:, ::-1] \n",
    "\n",
    "for i in range(n_clusters):\n",
    "    print(\"Cluster %d words:\" % i, end='')\n",
    "    \n",
    "    for ind in order_centroids[i, :20]: #replace with n words per cluster\n",
    "        print(' %s,' % terms[ind],end='')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counts of documents per each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    74\n",
       "2    31\n",
       "0     9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(km.labels_).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchichal clustering using stories did not work. Use cosine metric for distance as we do not want the length of the story to influence the clustering.\n",
    "I tried to use age, and nonprofit_cat as labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizerHC(doc):\n",
    "\n",
    "    stop = []\n",
    "    #stopwords.words('english')\n",
    "    stop += ['.', ',', '(', ')', \"'\", '\"']\n",
    "    \n",
    "    tokenized = []\n",
    "    words = word_tokenize(doc)\n",
    "    for w in words:\n",
    "        if re.match(\"([a-zA-Z]+'[a-zA-Z]+)|([a-zA-Z]+')|([a-zA-Z]+)\",w):\n",
    "            if w not in stop:\n",
    "                tokenized.append(w)\n",
    "    return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = df.storyclean\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words= None,ngram_range=(3, 3), tokenizer = mytokenizerHC,\n",
    "                             encoding='latin-1', max_features=1000, min_df=0.01, max_df=0.85)\n",
    "tf = vectorizer.fit(data)\n",
    "tf_matrix = tf.fit_transform(data).toarray()\n",
    "\n",
    "linkage_matrix = linkage(tf_matrix, method='weighted', metric='cosine')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "fig, ax = plt.subplots(figsize=(20, 8)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"top\", labels = df.nonprofit_cat);\n",
    "plt.title('Dendrogram of GivingTuesday Stories.', fontsize = 22)\n",
    "plt.xticks(rotation=15,fontsize =17)\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Analysis - topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories = df.storyclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizerLDA(doc):\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([s.capitalize() for s in stop])\n",
    "    stop += ['.', ',', '(', ')', \"'\", '\"', 'ha', 'u', 'would', 'one']\n",
    "    \n",
    "    tokenized = []\n",
    "    words = word_tokenize(doc)\n",
    "    for w in words:\n",
    "        if re.match(\"([a-zA-Z]+'[a-zA-Z]+)|([a-zA-Z]+')|([a-zA-Z]+)\",w):\n",
    "            if w not in stop:\n",
    "                tokenized.append(w)\n",
    "    return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {0}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sorted_num_of_docs_per_topic(corpus, fitted_model, tf_matrix):\n",
    "    topics = []\n",
    "    docs_by_topics_df = pd.DataFrame(fitted_model.transform(tf_matrix))\n",
    "    for row, val in docs_by_topics_df.iterrows():\n",
    "        topics.append(val.argmax())\n",
    "    result = pd.DataFrame(pd.Series(topics).value_counts()).reset_index()\n",
    "    result.columns = ['topic_num', 'docs_num']\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Males."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_features = 10000\n",
    "n_topics = 3\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "280"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "males = df[df.genderclean == 'Male']\n",
    "stories = males.storyclean\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.7, min_df=2, ngram_range=(1, 1), max_features=n_features, tokenizer = mytokenizerLDA)\n",
    "tf = tf_vectorizer.fit_transform(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features ..\n",
      "Topic 0:\n",
      "cancer family year life help time people child kid disease many give foundation could friend animal money day support like\n",
      "Topic 1:\n",
      "life community school student program give child year organization help world giving time people family opportunity need project many way\n",
      "Topic 2:\n",
      "year time day people help many life could get like family back need work home every food volunteer world first\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features ..\")\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=200,\n",
    "                                learning_method='batch', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num</th>\n",
       "      <th>docs_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_num  docs_num\n",
       "0          1       130\n",
       "1          2        86\n",
       "2          0        64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_num_of_docs_per_topic(stories, lda, tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Females."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mytokenizerLDA(doc):\n",
    "    \n",
    "    stop = stopwords.words('english')\n",
    "    stop.extend([s.capitalize() for s in stop])\n",
    "    stop += ['.', ',', '(', ')', \"'\", '\"', 'wa', 'ha', 'give', 'one', 'could', 'u', 'would']\n",
    "    \n",
    "    tokenized = []\n",
    "    words = word_tokenize(doc)\n",
    "    for w in words:\n",
    "        if re.match(\"([a-zA-Z]+'[a-zA-Z]+)|([a-zA-Z]+')|([a-zA-Z]+)\",w):\n",
    "            if w not in stop:\n",
    "                tokenized.append(w)\n",
    "    return(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1041"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "females = df[df.genderclean == 'Female']\n",
    "stories = females.storyclean\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 10000\n",
    "n_topics = 8\n",
    "n_top_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "Fitting LDA models with tf features ..\n",
      "Topic 0:\n",
      "child community school help program organization year family girl support\n",
      "Topic 1:\n",
      "year life time family day cancer people help get know\n",
      "Topic 2:\n",
      "time life room bird israel take home experience building able\n",
      "Topic 3:\n",
      "riding school music firefighter student program camp year kid deaf\n",
      "Topic 4:\n",
      "nicu community de day support womb neonatal free world foundation\n",
      "Topic 5:\n",
      "school student year child life time like world kid first\n",
      "Topic 6:\n",
      "help child life family dog people need many animal home\n",
      "Topic 7:\n",
      "test disease wildlife center psp virginia alzheimers eagle cure hope\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.7, min_df=2, ngram_range=(1, 1), max_features=n_features, tokenizer = mytokenizerLDA)\n",
    "tf = tf_vectorizer.fit_transform(stories)\n",
    "\n",
    "print(\"Fitting LDA models with tf features ..\")\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=200,\n",
    "                                learning_method='batch', learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_num</th>\n",
       "      <th>docs_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic_num  docs_num\n",
       "0          0       274\n",
       "1          6       271\n",
       "2          1       264\n",
       "3          5       195\n",
       "4          7        12\n",
       "5          4        12\n",
       "6          2         7\n",
       "7          3         6"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_num_of_docs_per_topic(stories, lda, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#readability, textstat packages - readability measures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
